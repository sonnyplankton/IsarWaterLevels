{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Analysing water levels of the river Isar\n",
    "\n",
    "In light of the recent flooding catastrophe with more then 170 dead, my interest sparked in monitoring and maybe predicting the waterlevels of the river in my hometown Munich, the river Isar.\n",
    "\n",
    "In this project you will get an idea how i refine my workflow and the identification of interesting data. I will also improve my skills in webscraping with beautiful soup and time series analysis.\n",
    "\n",
    "The project is structured in three main parts:\n",
    "\n",
    "1. Problem formulation and subject understanding\n",
    "2. Datasource identification and webscraping\n",
    "3. Data wrangling and time series analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Loading packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "\n",
    "now = datetime.today()\n",
    "\n",
    "import os\n",
    "from os import walk\n",
    "dirname = os.getcwd()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Script handling parameters\n",
    "scraping = True\n",
    "concatenating = True\n",
    "deleting_scraped_files = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Project goal and motivation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I started this project to get familiar with two skillsets in my data science learning journey:\n",
    "\n",
    "Tools and worflow technologies\n",
    "- Visual Studio Code\n",
    "- Github\n",
    "- Python scripting\n",
    "\n",
    "Data science subject\n",
    "- Time Series analysis\n",
    "- Time Series forecasting\n",
    "\n",
    "Also i see it as a portfolio project, highlighting my current skills in the above mentioned subjects, story telling and general Python programming. I will probably  \n",
    "use a lot ouf resources from practitioners, learners, professionals and amateures. For this reason i want to share my project and the isights i've gained with you and  \n",
    "i'm more then curios about your comments, suggestions and enhancements.\n",
    "\n",
    "Acknowledgements"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Problem formulation and subject understanding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The isar is a 292,3 kilometer long river that originates in the very north of Austria. The river crosses the capital of bavaria Munich until it merges with the Donau river south of Deggendorf. [Source: wikipedia.com/isar]\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Data collecting phase"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1. Data Source identification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The bavarian ministry for envoironment runs the so called \"Hochwassernachrichtendienst\" HND where the water levels of several rivers and lakes are provided. Additionally there are information on precipication and others.\n",
    "\n",
    "We focus on scraping the water levels first. The site is structured as follows\n",
    "\n",
    "    Basic Link:           https://www.hnd.bayern.de/pegel/meldestufen/isar/tabellen\n",
    "    Additional Parameter: ?days=0&hours=1\n",
    "\n",
    "We can address the levels per day for the last 30 days while 0 is today and 29 the oldest. The data is provided on an hourly interval from 0 to 23. For now, we don't need the most recent data, but this might change when upgrading to a more sophisticated scraping approach. At the moment we focus on getting a basic scrapping to work."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#Web scraping properties\n",
    "#link = \"https://www.hnd.bayern.de/pegel/meldestufen/isar/tabellen?days=0&hours=1\"\n",
    "basic_link = \"https://www.hnd.bayern.de/pegel/tabellen\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2. Web scraping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Currently we scrap the full month of data and discarge the duplicates when we merge the DataFrames. This leads to a unneccessary long scraping time.  \n",
    "In order to reduce the scapring time we want to determine which days need to be scraped by comparing the difference between the recent data and the   \n",
    "current date."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#list history file and read as df\n",
    "for file in os.listdir():\n",
    "    if 'isar_' in file:\n",
    "        hist_pegel = pd.read_csv(file)\n",
    "        last_scraped_datetime = pd.to_datetime(hist_pegel['Datum Zeit'][0], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        #determine the range of days to scrape\n",
    "        days_scrape  = [i for i in range(0,(now.day - last_scraped_datetime.day + 1))]\n",
    "    else:\n",
    "        #full scrape if no history available\n",
    "        hist_pegel = None\n",
    "        days_scrape  = [i for i in range(23,31)]\n",
    "\n",
    "\n",
    "#full hours to scrape\n",
    "hours_scrape = [i for i in range(0,24)]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "#File handling properties\n",
    "von_string = str(date.today() - timedelta(days_scrape[-1]))\n",
    "bis_string = str(date.today() - timedelta(days_scrape[0]))\n",
    "path = dirname + \"/ScrapingData/\"\n",
    "\n",
    "save_string = 'bay_river_pegel' + '_' + von_string +  '_bis_' + bis_string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, pandas 'read_html' class is a very powerful tool to get html tabels quickly into a dataframe. We now automate this approach to scrape the full history of the water levels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#Web scraping\n",
    "if scraping:\n",
    "\n",
    "    df_water_levels_scrape = pd.DataFrame()\n",
    "    \n",
    "    #add handling for most recent data\n",
    "    for day in days_scrape:\n",
    "        #when today, only scrape to current hour\n",
    "        if day == 0:\n",
    "\n",
    "            for hour in hours_scrape:\n",
    "                if hour <= (now.hour - 1):\n",
    "                    scrape_link = basic_link + '?days=' + str(day) + '&hours=' + str(hour)\n",
    "                    water_level = pd.read_html(scrape_link)\n",
    "                    df_water_levels_scrape = df_water_levels_scrape.append(water_level[0])\n",
    "                    \n",
    "        #when past days, scrape full hours\n",
    "        else:\n",
    "            for hour in hours_scrape:\n",
    "                scrape_link = basic_link + '?days=' + str(day) + '&hours=' + str(hour)\n",
    "                water_level = pd.read_html(scrape_link)\n",
    "                df_water_levels_scrape = df_water_levels_scrape.append(water_level[0])\n",
    "\n",
    "    #df_water_levels_scrape['Datum Zeit'] = pd.to_datetime(df_water_levels_scrape['Datum Zeit'], format='%d.%m.%Y, %H:%M')\n",
    "    df_water_levels_scrape.to_csv((path + save_string + '.csv'), index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#concat files in the scraping folder into a single df\n",
    "\n",
    "if concatenating == True:\n",
    "    #list files in the scraping folder and concat to single df\n",
    "    scraped_files       = os.listdir(path)\n",
    "    df_from_each_file   = (pd.read_csv(path + f) for f in scraped_files)\n",
    "    concatenated_df     = pd.concat(df_from_each_file, axis=0, ignore_index=True)\n",
    "\n",
    "    #sort and drop duplicates\n",
    "    concatenated_df     = concatenated_df.sort_values(by=['Datum Zeit'], ascending = False)\n",
    "    concatenated_df     = concatenated_df.drop_duplicates()\n",
    "\n",
    "    #concat hist and recent scraping\n",
    "    concatenated_df_2     = pd.concat([hist_pegel, concatenated_df], axis=0, ignore_index=True)\n",
    "    concatenated_df_2     = concatenated_df_2.sort_values(by=['Datum Zeit'], ascending = False)\n",
    "\n",
    "    #Clean up by dropping duplicates in observation point and datetime\n",
    "    concatenated_df_2   = concatenated_df_2.drop_duplicates(['Messstelle','Datum Zeit'])\n",
    "    #concatenated_df_2['Datum Zeit'] = pd.to_datetime(concatenated_df_2['Datum Zeit'], format='%Y-%m-%d %H:%M:%S')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since our scraping works as intended, we take the time to investigate the outcome. In theory, there should be 24 obersvations per day and oberservation point. We check this by saving an oberservation point to a dedicated DataFrame and group by date."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "waterlevel_sylvenstein = concatenated_df_2[concatenated_df_2['Messstelle'] == 'Sylvenstein']\n",
    "waterlevel_straubing = concatenated_df_2[concatenated_df_2['Messstelle'] == 'Straubing']\n",
    "#waterlevel_sylvenstein.groupby(by=waterlevel_sylvenstein['Datum Zeit'].dt.date).count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "waterlevel_sylvenstein"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Messstelle</th>\n",
       "      <th>Gewässer</th>\n",
       "      <th>Datum Zeit</th>\n",
       "      <th>Wasser­stand [cm]</th>\n",
       "      <th>Änderung seit 2 Std. [cm]</th>\n",
       "      <th>Abfluss [m³/s]</th>\n",
       "      <th>Melde­stufe</th>\n",
       "      <th>Jähr­lichkeit</th>\n",
       "      <th>Vorher­sage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Sylvenstein</td>\n",
       "      <td>Isar</td>\n",
       "      <td>29.08.2021, 23:00</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Sylvenstein</td>\n",
       "      <td>Isar</td>\n",
       "      <td>29.08.2021, 22:00</td>\n",
       "      <td>261</td>\n",
       "      <td>+1</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>Sylvenstein</td>\n",
       "      <td>Isar</td>\n",
       "      <td>29.08.2021, 21:00</td>\n",
       "      <td>261</td>\n",
       "      <td>+1</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>Sylvenstein</td>\n",
       "      <td>Isar</td>\n",
       "      <td>29.08.2021, 20:00</td>\n",
       "      <td>260</td>\n",
       "      <td>-1</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>Sylvenstein</td>\n",
       "      <td>Isar</td>\n",
       "      <td>29.08.2021, 19:00</td>\n",
       "      <td>260</td>\n",
       "      <td>-1</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48196</th>\n",
       "      <td>Sylvenstein</td>\n",
       "      <td>Isar</td>\n",
       "      <td>22.08.2021, 04:00</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48467</th>\n",
       "      <td>Sylvenstein</td>\n",
       "      <td>Isar</td>\n",
       "      <td>22.08.2021, 03:00</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48719</th>\n",
       "      <td>Sylvenstein</td>\n",
       "      <td>Isar</td>\n",
       "      <td>22.08.2021, 02:00</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49028</th>\n",
       "      <td>Sylvenstein</td>\n",
       "      <td>Isar</td>\n",
       "      <td>22.08.2021, 01:00</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49224</th>\n",
       "      <td>Sylvenstein</td>\n",
       "      <td>Isar</td>\n",
       "      <td>22.08.2021, 00:00</td>\n",
       "      <td>255</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Messstelle Gewässer         Datum Zeit Wasser­stand [cm]  \\\n",
       "134    Sylvenstein     Isar  29.08.2021, 23:00               261   \n",
       "264    Sylvenstein     Isar  29.08.2021, 22:00               261   \n",
       "521    Sylvenstein     Isar  29.08.2021, 21:00               261   \n",
       "775    Sylvenstein     Isar  29.08.2021, 20:00               260   \n",
       "1037   Sylvenstein     Isar  29.08.2021, 19:00               260   \n",
       "...            ...      ...                ...               ...   \n",
       "48196  Sylvenstein     Isar  22.08.2021, 04:00               255   \n",
       "48467  Sylvenstein     Isar  22.08.2021, 03:00               255   \n",
       "48719  Sylvenstein     Isar  22.08.2021, 02:00               255   \n",
       "49028  Sylvenstein     Isar  22.08.2021, 01:00               255   \n",
       "49224  Sylvenstein     Isar  22.08.2021, 00:00               255   \n",
       "\n",
       "      Änderung seit 2 Std. [cm] Abfluss [m³/s] Melde­stufe Jähr­lichkeit  \\\n",
       "134                           0            236           0           ---   \n",
       "264                          +1            236           0           ---   \n",
       "521                          +1            236           0           ---   \n",
       "775                          -1            228           0           ---   \n",
       "1037                         -1            228           0           ---   \n",
       "...                         ...            ...         ...           ...   \n",
       "48196                         0            184           0           ---   \n",
       "48467                         0            184           0           ---   \n",
       "48719                         0            184           0           ---   \n",
       "49028                         0            184           0           ---   \n",
       "49224                         0            184           0           ---   \n",
       "\n",
       "      Vorher­sage  \n",
       "134           NaN  \n",
       "264           NaN  \n",
       "521           NaN  \n",
       "775           NaN  \n",
       "1037          NaN  \n",
       "...           ...  \n",
       "48196         NaN  \n",
       "48467         NaN  \n",
       "48719         NaN  \n",
       "49028         NaN  \n",
       "49224         NaN  \n",
       "\n",
       "[192 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The scraped data is indeed what we have expected, exept for the last day, which is the current date and scraping is still a work in progress.\n",
    "\n",
    "Lastly we take a look at a complete day and see if we spot any mistakes whatsoever."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "concatenated_df_2['Messstelle'].unique()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Nittenau', 'Inkofen', 'Fürstenfeldbruck', 'Stegen',\n",
       "       'Partenkirchen (alt)', 'Beuerberg', 'Krotzenburg', 'Erlabrück',\n",
       "       'Oberheßbach', 'Poppenlauer', 'Laubendorf', 'Kreppendorf',\n",
       "       'Emskirchen', 'Büg', 'Erlangen', 'Hollfeld', 'Rappoldshofen',\n",
       "       'Laufermühle', 'Röbersdorf', 'Vorra', 'Arnstein',\n",
       "       'Bad Königshofen im Grabfeld', 'Salz',\n",
       "       'Bad Kissingen Prinzreg.-bau', 'Wolfsmünster', 'Gollmuthhausen',\n",
       "       'Nordheim vor der Rhön', 'Unsleben', 'Wechterswinkel', 'Stöckach',\n",
       "       'Ampermoching', 'Oberammergau', 'München', 'Peißenberg',\n",
       "       'Bad Tölz Brücke', 'Lenggries', 'Sylvenstein', 'Kötzting',\n",
       "       'Garmisch u. d. Partnachmündung', 'Lohberg', 'Sägmühle',\n",
       "       'Teisnach', 'Zwiesel', 'Marienthal', 'Kienhof', 'Cham', 'Chamerau',\n",
       "       'Pulling', 'Garmisch o. d. Partnachmündung', 'Schlehdorf',\n",
       "       'Mühldorf', 'Oberfinning Seepegel', 'Kraiburg',\n",
       "       'Rosenheim o.d. Mangfallmündung', 'Kalteneck', 'Dietelskirchen',\n",
       "       'Vilsbiburg', 'Grafenmühle', 'Rottersdorf', 'Hohenkammer',\n",
       "       'Leutstetten', 'Raisting', 'Kochel', 'Lauf', 'Kösfeld',\n",
       "       'Neukenroth', 'Steinberg', 'Mitteldachstetten', 'Weinzierlein',\n",
       "       'Eching', 'Oberfinning Speicherabgabe', 'Aichach Blauer Steg',\n",
       "       'Greiendorf', 'Ansbach', 'Scheinfeld', 'Hiltmannsdorf HW',\n",
       "       'Neumühle', 'Arzberg', 'Pettstadt', 'Mittelsinn', 'Bockenfeld',\n",
       "       'Archshofen', 'Weilbach', 'Michelstadt', 'Hainstadt',\n",
       "       'Schöllkrippen', 'Bad Vilbel', 'Bieberehren', 'Hof', 'Schwabach',\n",
       "       'Berg', 'Ingolstadt Luitpoldstraße', 'Pommelsbrunn',\n",
       "       'Bad Kissingen Golfplatz', 'Wernfels Kläranlage', 'Fronhof',\n",
       "       'Bad Brückenau', 'Starnberg', 'Oberaudorf', 'Rasch', 'Steinach',\n",
       "       'Fürth am Berg', 'Schönstädt Seepegel', 'Schönstädt', 'Coburg',\n",
       "       'Mönchröden', 'Lohr', 'Güntersthal', 'Hüttendorf',\n",
       "       'Roth Kläranlage', 'Katzwang', 'Mühlstetten', 'Wendelstein',\n",
       "       'Schärding', 'Michelfeld', 'Schweinhof', 'Birkenfeld',\n",
       "       'Oberthulba', 'Frauenkreuz', 'Muggendorf', 'Eschenlohe Brücke',\n",
       "       'Wasserburg', 'Leucherhof', 'Schenkenau', 'Golling', 'Innsbruck',\n",
       "       'Rotholz', 'Puppling', 'Nürnberg Lederersteg', 'Freising',\n",
       "       'Unterköblitz', 'Haslach Werksabfluss', 'Passau', 'Engen',\n",
       "       'Kelheim', 'Vilshofen', 'Hofkirchen', 'Deggendorf', 'Pfelling',\n",
       "       'Straubing', 'Regensburg Eiserne Brücke', 'Oberndorf', 'Neuburg',\n",
       "       'Augsburg u. d. Wertachmündung', 'Donauwörth', 'Dillingen',\n",
       "       'Günzburg u. d. Günzmündung', 'Neu Ulm, Bad Held', 'Schwabelweis',\n",
       "       'Amberg', 'Unterlangenstadt', 'Passau Ilzstadt', 'Sonthofen',\n",
       "       'Kempten', 'Wiblingen', 'Haunstetten', 'Landsberg', 'Lechbruck',\n",
       "       'Achsheim', 'Fischach', 'Deuerling', 'Münchshofen', 'Rödenweiler',\n",
       "       'Heitzenhofen', 'Landshut Birket', 'Treuchtlingen', 'Aha', 'Thann',\n",
       "       'Binzwangen', 'Geisenfeld', 'Mühlried', 'Mainburg',\n",
       "       'Manching (Ort)', 'Beilngries oberh. der Sulz', 'Dasing', 'Mering',\n",
       "       'Augsburg Oberhausen', 'Türkheim', 'Biessenhofen', 'Eichstätt',\n",
       "       'Parsberg', 'Bechhofen', 'Pressath', 'Sebastianskapelle',\n",
       "       'Vilseck', 'Warnbach', 'Eixendorf Speicherabfluss',\n",
       "       'Rötz Speicherzufluss', 'Trausnitz u. d. Mühle', 'Erbendorf',\n",
       "       'Mettendorf', 'Neustadt', 'Windischeschenbach', 'Fleinhausen',\n",
       "       'Aunkofen', 'Harburg', 'Obernau', 'Mertseespeicher', 'St. Quirin',\n",
       "       'Frankfurt a.M. Osthafen', 'Untersteinach', 'Gampelmühle',\n",
       "       'Bayreuth', 'Creußen', 'Wirsberg', 'Ködnitz', 'Bad Berneck',\n",
       "       'Raunheim', 'Mainleus', 'Kleinheubach', 'Rosenheim', 'Faulbach',\n",
       "       'Steinbach', 'Würzburg', 'Schweinfurt Neuer Hafen', 'Trunstadt',\n",
       "       'Kemmern', 'Schwürbitz', 'Feldolling', 'Passau Marienbrücke',\n",
       "       'Stettkirchen', 'Furth im Wald', 'Plattling', 'Landau', 'Birnbach',\n",
       "       'Rieblich', 'Schliersee', 'Brodhausen', 'Miesbach', 'Reichenbach',\n",
       "       'Offingen', 'Hasberg', 'Nattenhausen', 'Lauben', 'Reckenberg',\n",
       "       'Hahnbach', 'Weilheim', 'Kastl', 'Stein bei Altenmarkt',\n",
       "       'Postmünster', 'Linden', 'Kinning', 'Gerolfingen', 'Siezenheim',\n",
       "       'Ruhstorf', 'Stauden', 'Engfurt', 'Seebruck', 'Burgkirchen',\n",
       "       'Staudach', 'Trostberg', 'Hochberg', 'Laufen Siegerstetter Keller',\n",
       "       'Burghausen', 'Obergäu', 'Unterjettenberg'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are happy with the results and save the dataframe to the disk."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Persistently save data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Obviously, we want to save our current efforts persistently to svae the historical data we have put so much thought and effort in. We use .csv for now and also delete the last complete file to keep things tidy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#delete current history file\n",
    "os.remove(file)\n",
    "\n",
    "#determine filename from content\n",
    "end_date            = concatenated_df_2['Datum Zeit'].iloc[0].strftime('%Y-%m-%d')\n",
    "start_date          = concatenated_df_2['Datum Zeit'].iloc[-1].strftime('%Y-%m-%d')\n",
    "save_string2        = 'isar_pegel' + '_' + start_date +  '_bis_' + end_date\n",
    "\n",
    "#saving concatenated df as .csv to disk\n",
    "concatenated_df_2.to_csv((save_string2 + '.csv'), index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "os.stat(save_string2 + '.csv')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "os.stat_result(st_mode=33188, st_ino=37244180, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=1702925, st_atime=1632249091, st_mtime=1632249091, st_ctime=1632249091)"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#deleting scraped data\n",
    "if deleting_scraped_files == True:\n",
    "    for f in scraped_files:\n",
    "        os.remove(path + f)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we are now able to automatically scrape and save the waterlevels, lets take a look at the date we get."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Data understanding phase"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we can work with the data we need to understand the collected data first. I have separated this task into two main parts;\n",
    "\n",
    "- The quantitaive data understanding  \n",
    "Here we want to now how data data looks in the first place. Which variabels are in the data, how many observations, NaN values, data types, statistical information in numerical and unique values\n",
    "in categorical variables.  \n",
    "  \n",
    "  \n",
    "- The qualitative data understanding  \n",
    "Goal is to really understand the data from an domain point of view. How are different rivers, measuring points etc. are connected, what do cetrain variables mean.\n",
    "These questions are best answered by consulting the website (FAQ, reading material, documentation etc.) or interviewing experts in the field. These insights will\n",
    "help us further improve further analysis and forecasting."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start with loading the latest data into a dataframe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Load the complete csv to dataframe\n",
    "for file in os.listdir():\n",
    "    if 'isar_pegel_' in file:\n",
    "        waterlevel_hist = pd.read_csv(file)\n",
    "    \n",
    "    else:\n",
    "        waterlevel_hist = None\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "waterlevel_hist.head()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-aa9f02a6e3b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwaterlevel_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "waterlevel_hist.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27125 entries, 0 to 27124\n",
      "Data columns (total 9 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Messstelle                 27125 non-null  object \n",
      " 1   Gewässer                   27125 non-null  object \n",
      " 2   Datum Zeit                 27125 non-null  object \n",
      " 3   Wasser­stand [cm]          27125 non-null  int64  \n",
      " 4   Änderung seit 2 Std. [cm]  27125 non-null  int64  \n",
      " 5   Abfluss [m³/s]             27125 non-null  object \n",
      " 6   Melde­stufe                27125 non-null  int64  \n",
      " 7   Jähr­lichkeit              27125 non-null  object \n",
      " 8   Vorher­sage                0 non-null      float64\n",
      "dtypes: float64(1), int64(3), object(5)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "waterlevel_hist['Datum Zeit'] = pd.to_datetime(waterlevel_hist['Datum Zeit'], format='%Y-%m-%d %H:%M:%S')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "waterlevel_messtellen = waterlevel_hist['Messstelle'].unique()\n",
    "waterlevel_gewässer = waterlevel_hist['Gewässer'].unique()\n",
    "waterlevel_meldestufe = waterlevel_hist['Melde­stufe'].unique()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(waterlevel_messtellen)\n",
    "print(waterlevel_gewässer)\n",
    "print(waterlevel_meldestufe)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Partenkirchen (alt)' 'Garmisch o. d. Partnachmündung' 'Fürstenfeldbruck'\n",
      " 'Oberammergau' 'Inkofen' 'Ampermoching' 'Stegen' 'Beuerberg' 'Peißenberg'\n",
      " 'Weilheim' 'Raisting' 'Oberfinning Seepegel' 'Kochel' 'Schlehdorf'\n",
      " 'Garmisch u. d. Partnachmündung' 'Oberfinning Speicherabgabe' 'Plattling'\n",
      " 'Sylvenstein' 'Leutstetten' 'Hohenkammer' 'Berg' 'Eschenlohe Brücke'\n",
      " 'Starnberg' 'Eching' 'Landau' 'Lenggries' 'Puppling' 'München' 'Freising'\n",
      " 'Landshut Birket' 'Bad Tölz Brücke']\n",
      "['Partnach' 'Loisach' 'Amper' 'Ammer' 'Rott' 'Windachspeicher' 'Windach'\n",
      " 'Isar' 'Würm' 'Glonn' 'Sempt' 'Starnberger See' 'Ammersee']\n",
      "[0 4 1 3 2]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "waterlevel_isar = waterlevel_hist[waterlevel_hist['Gewässer'] == 'Isar']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#datum_list = waterlevel_sylvenstein['Datum Zeit'].dt.floor('d').value_counts()()\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "waterlevel_isar['Messstelle'].unique()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Plattling', 'Sylvenstein', 'Landau', 'Lenggries', 'Puppling',\n",
       "       'München', 'Freising', 'Landshut Birket', 'Bad Tölz Brücke'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "e27770a5904c05f346266c3078d2f1c16671dbcdfa3785635894e45a4a43d3d6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}