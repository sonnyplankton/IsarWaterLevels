{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Analysing water levels of the river Isar\n",
    "\n",
    "In light of the recent flooding catastrophe with more then 170 dead, my interest sparked in monitoring and maybe predicting the waterlevels of the river in my hometown Munich, the river Isar.\n",
    "\n",
    "In this project you will follow how i refine my workflow and the identification of interesting data data. I will also improve my skills in webscraping with beautiful soup and time series analysis.\n",
    "\n",
    "The project is structured in three main parts:\n",
    "\n",
    "1. Problem formulation and subject understanding\n",
    "2. Datasource identification and webscraping\n",
    "3. Data wrangling and time series analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Loading packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "\n",
    "import os\n",
    "from os import walk\n",
    "dirname = os.getcwd()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "source": [
    "#Script handling parameters\n",
    "scraping = False\n",
    "concatenating = True\n",
    "deleting_scraped_files = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Problem formulation and subject understanding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The isar is a 292,3 kilometer long river that originates in the very north of Austria. The river crosses the capital of bavaria Munich until it merges with the Donau river south of Deggendorf. [Source: wikipedia.com/isar]\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data Source identification and webscraping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The bavarian ministry for envoironment runs the so called \"Hochwassernachrichtendienst\" HND where the water levels of several rivers and lakes are provided. Additionally there are information on precipication and others.\n",
    "\n",
    "We focus on scraping the water levels first. The site is structured as follows\n",
    "\n",
    "    Basic Link:           https://www.hnd.bayern.de/pegel/meldestufen/isar/tabellen\n",
    "    Additional Parameter: ?days=0&hours=1\n",
    "\n",
    "We can address the levels per day for the last 30 days while 0 is today and 29 the oldest. The data is provided on an hourly interval from 0 to 23. For now, we don't need the most recent data, but this might change when upgrading to a more sophisticated scraping approach. At the moment we focus on getting a basic scrapping to work."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "source": [
    "#we create two lists for the days and hours we want to scrape.\n",
    "days_scrape  = [i for i in range(1,30)]\n",
    "hours_scrape = [i for i in range(0,24)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "source": [
    "#Web scraping properties\n",
    "link = \"https://www.hnd.bayern.de/pegel/meldestufen/isar/tabellen?days=0&hours=1\"\n",
    "basic_link = \"https://www.hnd.bayern.de/pegel/meldestufen/isar/tabellen\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "source": [
    "#File handling properties\n",
    "von_string = str(date.today() - timedelta(days_scrape[-1]))\n",
    "bis_string = str(date.today())\n",
    "path = dirname + \"/ScrapingData/\"\n",
    "\n",
    "save_string = 'isar_pegel' + '_' + von_string +  '_bis_' + bis_string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, pandas 'read_html' class is a very powerful tool to get html tabels quickly into a dataframe. We now automate this approach to scrape the full history of the water levels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "source": [
    "#Web scraping\n",
    "if scraping:\n",
    "\n",
    "    df_water_levels_scrape = pd.DataFrame()\n",
    "\n",
    "    for day in days_scrape: \n",
    "        for hour in hours_scrape:\n",
    "            scrape_link = basic_link + '?days=' + str(day) + '&hours=' + str(hour)\n",
    "            water_level = pd.read_html(scrape_link)\n",
    "            df_water_levels_scrape = df_water_levels_scrape.append(water_level[0])\n",
    "\n",
    "    df_water_levels_scrape['Datum Zeit'] = pd.to_datetime(df_water_levels_scrape['Datum Zeit'], format='%d.%m.%Y, %H:%M')\n",
    "    df_water_levels_scrape.to_csv(path + save_string + '.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "source": [
    "#concat files in the scraping folder into a single df\n",
    "\n",
    "if concatenating == True:\n",
    "    \n",
    "    #list history file and read as df\n",
    "    for file in os.listdir():\n",
    "        if 'isar_pegel_' in file:\n",
    "            hist_pegel = pd.read_csv(file)\n",
    "        else:\n",
    "            hist_pegel = None\n",
    "\n",
    "    #list files in the scraping folder and concat to history + concat to df\n",
    "    scraped_files       = os.listdir(path)\n",
    "    df_from_each_file   = (pd.read_csv(path + f) for f in scraped_files)\n",
    "    concatenated_df     = pd.concat(df_from_each_file, ignore_index=True)\n",
    "\n",
    "    concatenated_df.sort_values(by=['Datum Zeit'], ascending = False)\n",
    "\n",
    "    #concat hist and recent scraping\n",
    "    if hist_pegel != None:\n",
    "        concatenated_df     = pd.concat([hist_pegel, concatenated_df], ignore_index=True)\n",
    "    else:\n",
    "        concatenated_df     = concatenated_df\n",
    "\n",
    "    #saving concatenated df to file\n",
    "    end_date            = concatenated_df['Datum Zeit'].iloc[0][:10]\n",
    "    start_date          = concatenated_df['Datum Zeit'].iloc[-1][:10]\n",
    "    save_string2        = 'isar_pegel' + '_' + start_date +  '_bis_' + end_date\n",
    "\n",
    "    concatenated_df.drop_duplicates()\n",
    "\n",
    "    concatenated_df.to_csv(save_string2 + '.csv')\n",
    "\n",
    "    #deleting scraped data\n",
    "    if deleting_scraped_files == True:\n",
    "        for f in scraped_files:\n",
    "            os.remove(path + f)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "e27770a5904c05f346266c3078d2f1c16671dbcdfa3785635894e45a4a43d3d6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}