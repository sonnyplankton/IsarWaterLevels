{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Analysing water levels of the river Isar\n",
    "\n",
    "In light of the recent flooding catastrophe with more then 170 dead, my interest sparked in monitoring and maybe predicting the waterlevels of the river in my hometown Munich, the river Isar.\n",
    "\n",
    "In this project you will follow how i refine my workflow and the identification of interesting data data. I will also improve my skills in webscraping with beautiful soup and time series analysis.\n",
    "\n",
    "The project is structured in three main parts:\n",
    "\n",
    "1. Problem formulation and subject understanding\n",
    "2. Datasource identification and webscraping\n",
    "3. Data wrangling and time series analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Loading packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import date\n",
    "\n",
    "import os\n",
    "from os import walk\n",
    "dirname = os.getcwd()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "source": [
    "#Script handling parameters\n",
    "scraping = True\n",
    "concatenating = True\n",
    "deleting_scraped_files = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Project goal and motivation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I started this project to get familiar with two skillsets in my data science learning journey:\n",
    "\n",
    "Tools and worflow technologies\n",
    "- Visual Studio Code\n",
    "- Github\n",
    "- Python scripting\n",
    "\n",
    "Data science subject\n",
    "- Time Series analysis\n",
    "- Time Series forecasting\n",
    "\n",
    "Also i see it as a portfolio project, highlighting my current skills in the above mentioned subjects, story telling and general Python programming. I will probably  \n",
    "use a lot ouf resources from practitioners, learners, professionals and amateures. For this reason i want to share my project and the isights i've gained with you and  \n",
    "i'm more then curios about your comments, suggestions and enhancements.\n",
    "\n",
    "Acknowledgements"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Problem formulation and subject understanding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The isar is a 292,3 kilometer long river that originates in the very north of Austria. The river crosses the capital of bavaria Munich until it merges with the Donau river south of Deggendorf. [Source: wikipedia.com/isar]\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Data collecting phase"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1. Data Source identification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The bavarian ministry for envoironment runs the so called \"Hochwassernachrichtendienst\" HND where the water levels of several rivers and lakes are provided. Additionally there are information on precipication and others.\n",
    "\n",
    "We focus on scraping the water levels first. The site is structured as follows\n",
    "\n",
    "    Basic Link:           https://www.hnd.bayern.de/pegel/meldestufen/isar/tabellen\n",
    "    Additional Parameter: ?days=0&hours=1\n",
    "\n",
    "We can address the levels per day for the last 30 days while 0 is today and 29 the oldest. The data is provided on an hourly interval from 0 to 23. For now, we don't need the most recent data, but this might change when upgrading to a more sophisticated scraping approach. At the moment we focus on getting a basic scrapping to work."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "source": [
    "#we create two lists for the days and hours we want to scrape.\n",
    "days_scrape  = [i for i in range(1,30)]\n",
    "hours_scrape = [i for i in range(0,24)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "source": [
    "#Web scraping properties\n",
    "link = \"https://www.hnd.bayern.de/pegel/meldestufen/isar/tabellen?days=0&hours=1\"\n",
    "basic_link = \"https://www.hnd.bayern.de/pegel/meldestufen/isar/tabellen\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "source": [
    "#File handling properties\n",
    "von_string = str(date.today() - timedelta(days_scrape[-1]))\n",
    "bis_string = str(date.today())\n",
    "path = dirname + \"/ScrapingData/\"\n",
    "\n",
    "save_string = 'isar_pegel' + '_' + von_string +  '_bis_' + bis_string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2. Web scraping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Currently we scrap the full month of data and discarge the duplicates when we merge the DataFrames. This leads to a unneccessary long scraping time.  \n",
    "In order to reduce the scapring time we want to determine which days need to be scraped by comparing the difference between the recent data and the   \n",
    "current date."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "source": [
    "#list history file and read as df\n",
    "for file in os.listdir():\n",
    "    if 'isar_pegel_' in file:\n",
    "        hist_pegel = pd.read_csv(file)\n",
    "\n",
    "        days_scrape  = [i for i in range(1,30)]\n",
    "        hours_scrape = [i for i in range(0,24)]\n",
    "    \n",
    "    else:\n",
    "        hist_pegel = None\n",
    "\n",
    "        days_scrape  = [i for i in range(1,30)]\n",
    "        hours_scrape = [i for i in range(0,24)]\n",
    "    \n",
    "    break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, pandas 'read_html' class is a very powerful tool to get html tabels quickly into a dataframe. We now automate this approach to scrape the full history of the water levels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "source": [
    "#Web scraping\n",
    "if scraping:\n",
    "\n",
    "    df_water_levels_scrape = pd.DataFrame()\n",
    "\n",
    "    for day in days_scrape: \n",
    "        for hour in hours_scrape:\n",
    "            scrape_link = basic_link + '?days=' + str(day) + '&hours=' + str(hour)\n",
    "            water_level = pd.read_html(scrape_link)\n",
    "            df_water_levels_scrape = df_water_levels_scrape.append(water_level[0])\n",
    "\n",
    "    df_water_levels_scrape['Datum Zeit'] = pd.to_datetime(df_water_levels_scrape['Datum Zeit'], format='%d.%m.%Y, %H:%M')\n",
    "    df_water_levels_scrape.to_csv((path + save_string + '.csv'), index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "source": [
    "#concat files in the scraping folder into a single df\n",
    "\n",
    "if concatenating == True:\n",
    "       \n",
    "\n",
    "    #list files in the scraping folder and concat to single df\n",
    "    scraped_files       = os.listdir(path)\n",
    "    df_from_each_file   = (pd.read_csv(path + f) for f in scraped_files)\n",
    "    concatenated_df     = pd.concat(df_from_each_file, axis=0, ignore_index=True)\n",
    "\n",
    "    #sort and drop duplicates\n",
    "    concatenated_df     = concatenated_df.sort_values(by=['Datum Zeit'], ascending = False)\n",
    "    concatenated_df     = concatenated_df.drop_duplicates()\n",
    "\n",
    "    #concat hist and recent scraping\n",
    "    concatenated_df_2     = pd.concat([hist_pegel, concatenated_df], axis=0, ignore_index=True)\n",
    "    concatenated_df_2     = concatenated_df_2.sort_values(by=['Datum Zeit'], ascending = False)\n",
    "\n",
    "    #saving concatenated df to file\n",
    "    end_date            = concatenated_df_2['Datum Zeit'].iloc[0][:10]\n",
    "    start_date          = concatenated_df_2['Datum Zeit'].iloc[-1][:10]\n",
    "    save_string2        = 'isar_pegel' + '_' + start_date +  '_bis_' + end_date\n",
    "\n",
    "    os.remove(file)\n",
    "\n",
    "    concatenated_df_2.drop_duplicates()\n",
    "\n",
    "    concatenated_df_2.to_csv((save_string2 + '.csv'), index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "source": [
    "#deleting scraped data\n",
    "if deleting_scraped_files == True:\n",
    "    for f in scraped_files:\n",
    "        os.remove(path + f)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we are now able to automatically scrape and save the waterlevels, lets take a look at the date we get."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Data understanding phase"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we can work with the data we need to understand the collected data first. I have separated this task into two main parts;\n",
    "\n",
    "- The quantitaive data understanding  \n",
    "Here we want to now how data data looks in the first place. Which variabels are in the data, how many observations, NaN values, data types, statistical information in numerical and unique values\n",
    "in categorical variables.  \n",
    "  \n",
    "  \n",
    "- The qualitative data understanding  \n",
    "Goal is to really understand the data from an domain point of view. How are different rivers, measuring points etc. are connected, what do cetrain variables mean.\n",
    "These questions are best answered by consulting the website (FAQ, reading material, documentation etc.) or interviewing experts in the field. These insights will\n",
    "help us further improve further analysis and forecasting."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start with loading the latest data into a dataframe."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "source": [
    "#Load the complete csv to dataframe\n",
    "for file in os.listdir():\n",
    "        if 'isar_pegel_' in file:\n",
    "            waterlevel_hist = pd.read_csv(file)\n",
    "    \n",
    "        else:\n",
    "            waterlevel_hist = None\n",
    "    \n",
    "        break\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "source": [
    "waterlevel_hist.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Messstelle Gewässer           Datum Zeit  Wasser­stand [cm]  \\\n",
       "0   Sylvenstein     Isar  2021-09-13 23:00:00                245   \n",
       "1    Schlehdorf  Loisach  2021-09-13 23:00:00                 97   \n",
       "2  Oberammergau    Ammer  2021-09-13 23:00:00                 47   \n",
       "3    Peißenberg    Ammer  2021-09-13 23:00:00                 68   \n",
       "4      Weilheim    Ammer  2021-09-13 23:00:00                 47   \n",
       "\n",
       "   Änderung seit 2 Std. [cm] Abfluss [m³/s]  Melde­stufe Jähr­lichkeit  \\\n",
       "0                          0            110            0           ---   \n",
       "1                          0            169            0           ---   \n",
       "2                          0            348            0           ---   \n",
       "3                          0            715            0           ---   \n",
       "4                         -1            977            0           ---   \n",
       "\n",
       "   Vorher­sage  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Messstelle</th>\n",
       "      <th>Gewässer</th>\n",
       "      <th>Datum Zeit</th>\n",
       "      <th>Wasser­stand [cm]</th>\n",
       "      <th>Änderung seit 2 Std. [cm]</th>\n",
       "      <th>Abfluss [m³/s]</th>\n",
       "      <th>Melde­stufe</th>\n",
       "      <th>Jähr­lichkeit</th>\n",
       "      <th>Vorher­sage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sylvenstein</td>\n",
       "      <td>Isar</td>\n",
       "      <td>2021-09-13 23:00:00</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schlehdorf</td>\n",
       "      <td>Loisach</td>\n",
       "      <td>2021-09-13 23:00:00</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oberammergau</td>\n",
       "      <td>Ammer</td>\n",
       "      <td>2021-09-13 23:00:00</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>348</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peißenberg</td>\n",
       "      <td>Ammer</td>\n",
       "      <td>2021-09-13 23:00:00</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>715</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Weilheim</td>\n",
       "      <td>Ammer</td>\n",
       "      <td>2021-09-13 23:00:00</td>\n",
       "      <td>47</td>\n",
       "      <td>-1</td>\n",
       "      <td>977</td>\n",
       "      <td>0</td>\n",
       "      <td>---</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 542
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "source": [
    "waterlevel_hist.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 69936 entries, 0 to 69935\n",
      "Data columns (total 9 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Messstelle                 69936 non-null  object \n",
      " 1   Gewässer                   69936 non-null  object \n",
      " 2   Datum Zeit                 69936 non-null  object \n",
      " 3   Wasser­stand [cm]          69936 non-null  int64  \n",
      " 4   Änderung seit 2 Std. [cm]  69936 non-null  int64  \n",
      " 5   Abfluss [m³/s]             69936 non-null  object \n",
      " 6   Melde­stufe                69936 non-null  int64  \n",
      " 7   Jähr­lichkeit              69936 non-null  object \n",
      " 8   Vorher­sage                0 non-null      float64\n",
      "dtypes: float64(1), int64(3), object(5)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "source": [
    "waterlevel_hist['Datum Zeit'] = pd.to_datetime(waterlevel_hist['Datum Zeit'], format='%Y-%m-%d %H:%M:%S')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "source": [
    "waterlevel_messtellen = waterlevel_hist['Messstelle'].unique()\n",
    "waterlevel_gewässer = waterlevel_hist['Gewässer'].unique()\n",
    "waterlevel_meldestufe = waterlevel_hist['Melde­stufe'].unique()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "source": [
    "print(waterlevel_messtellen)\n",
    "print(waterlevel_gewässer)\n",
    "print(waterlevel_meldestufe)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Sylvenstein' 'Schlehdorf' 'Oberammergau' 'Peißenberg' 'Weilheim'\n",
      " 'Raisting' 'Oberfinning Seepegel' 'Leutstetten' 'Berg' 'Bad Tölz Brücke'\n",
      " 'Starnberg' 'Eching' 'Oberfinning Speicherabgabe' 'Inkofen' 'Hohenkammer'\n",
      " 'Ampermoching' 'Eschenlohe Brücke' 'Stegen' 'Landau' 'Lenggries'\n",
      " 'Puppling' 'München' 'Freising' 'Landshut Birket' 'Fürstenfeldbruck'\n",
      " 'Plattling' 'Garmisch o. d. Partnachmündung'\n",
      " 'Garmisch u. d. Partnachmündung' 'Kochel' 'Beuerberg'\n",
      " 'Partenkirchen (alt)']\n",
      "['Isar' 'Loisach' 'Ammer' 'Rott' 'Windachspeicher' 'Würm' 'Sempt'\n",
      " 'Starnberger See' 'Windach' 'Amper' 'Glonn' 'Ammersee' 'Partnach']\n",
      "[0 1 3 2]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "source": [
    "print(file)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "isar_pegel_2021-08-13_bis_2021-09-13.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "e27770a5904c05f346266c3078d2f1c16671dbcdfa3785635894e45a4a43d3d6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}